{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NiIF6hD_bo0",
        "outputId": "1f62396a-fa9a-46c0-fabd-ac58c3324802"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install langdetect\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7MrNHuR6tt2"
      },
      "source": [
        "language_d ={\n",
        "    \"Telugu\" : \"te\",\n",
        "    \"Vietnamese\" : \"vi\",\n",
        "    \"Lithuanian\" : \"lt\",\n",
        "    \"French\": \"fr\",\n",
        "    \"Thai\": \"th\",\n",
        "    \"Czech\" : \"cs\",\n",
        "    \"Bengali\" : \"bn\",\n",
        "    \"Spanish\": \"es\",\n",
        "    \"Russian\": \"ru\",\n",
        "    'Urdu':'ur',\n",
        "    'Latvian':'lv',\n",
        "    'Italian':'it',\n",
        "    'Swahili':'sw,\n",
        "    'Ukrainian':'uk',\n",
        "    'Korean':'ko',\n",
        "    'English':'en', \n",
        "    'Romanian':'ro', \n",
        "    'Nepali':'ne', \n",
        "    'Indonesian':'id',\n",
        "    'marathi': 'mr', \n",
        "    'arabic': 'ar', \n",
        "    'hindi' : 'hi', \n",
        "    'german': 'de', \n",
        "    'persian': 'fa',\n",
        "    'estonian': 'et', \n",
        "    'polish': 'pl', \n",
        "    'sweden': 'sv',\n",
        "    \"Portugese\":\"pt\", \n",
        "    \"Turkish\": \"tr\", \n",
        "    \"Afrikaans\":\"af\", \n",
        "    \"Tamil\":\"ta\", \n",
        "    \"Dutch\":\"nl\", \n",
        "    \"Tagalog\":\"tl\", \n",
        "    \"Slovenian\":\"sl\"\n",
        "    \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qdb1U5qYsto"
      },
      "source": [
        "lang_dictionary = {\n",
        "    \"sv\":\"sv_SE\",\n",
        "    \"fi\":\"fi_FI\",\n",
        "    \"hi\":\"hi_IN\",\n",
        "    \"ro\":\"ro_RO\",\n",
        "    \"ko\":\"ko_KR\",\n",
        "    \"sl\":\"sl_SI\",\n",
        "    \"mr\":\"mr_IN\",\n",
        "    \"pt\":\"pt_XX\",\n",
        "    \"de\":\"de_DE\",\n",
        "    \"en\":\"en_XX\",\n",
        "    \"fa\": \"fa_IR\",\n",
        "    \"th\": \"th_TH\",\n",
        "    \"mk\": \"mk_MK\",\n",
        "    \"pl\": \"pl_PL\",\n",
        "    \"en\": \"en_XX\",\n",
        "    \"ru\": \"ru_RU\",\n",
        "    \"lv\": \"lv_LV\",\n",
        "    \"bn\": \"bn_IN\", \n",
        "    \"et\": \"et_EE\",\n",
        "    \"af\": \"af_ZA\",\n",
        "    \"he\":\"he_IL\",\n",
        "    \"tl\":\"tl_XX\",\n",
        "    \"hr\":\"hr_HR\",\n",
        "    \"ml\":\"ml_IN\",\n",
        "    \"es\":\"es_XX\",\n",
        "    \"it\":\"it_IT\",\n",
        "    \"nl\":\"nl_XX\",\n",
        "    \"ar\":\"ar_AR\",\n",
        "    \"tr\":\"tr_TR\",\n",
        "    \"ta\":\"ta_IN\",\n",
        "    \"te\" : \"te_IN\",\n",
        "    \"fr\" : \"fr_XX\",\n",
        "    \"ne\" : \"ne_NP\",\n",
        "    \"lt\" : \"lt_LT\",\n",
        "    \"ur\" : \"ur_PK\",\n",
        "    \"uk\" : \"uk_UA\",\n",
        "    \"sw\" : \"sw_KE\",\n",
        "    \"cs\" : \"cs_CZ\",\n",
        "    \"gu\" : \"gu_IN\"\n",
        "\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blNeyOTPdaKx"
      },
      "source": [
        "from langdetect import detect"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZQr0xf9cv2R",
        "outputId": "dbcd158c-c008-4a97-9cf6-d6e90bf083aa"
      },
      "source": [
        "out_list =[]\n",
        "text = [\"मुझे नाचना पसंद है\",\"कृपया भोजन शुरू कीजियै!\",\"എനിക്ക് മനസ്സിലായില്ല\",\"உங்கள் பெயர் என்ன?\"]\n",
        "for i in text:\n",
        "  #model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "  #tokenizer.src_lang = \"hi_IN\"\n",
        "  r = detect(i)\n",
        "  lang = lang_dictionary[r]\n",
        "  #############################\n",
        "  tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang = lang)\n",
        "  encoded_text = tokenizer(i, return_tensors=\"pt\")\n",
        "  if r==\"en\":\n",
        "    print(\"Text is already in ENGLISH\")\n",
        "  else:\n",
        "    \n",
        "    generated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
        "    out = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "    out = str(out).strip('][\\'')\n",
        "    out_list.append(out)\n",
        "\n",
        "out_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I like to dance.',\n",
              " 'Please start your meal!',\n",
              " '\"I don\\'t understand.\"',\n",
              " 'What is your name?']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}